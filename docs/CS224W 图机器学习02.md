# Chapter2 CS224W 图机器学习2

- **斯坦福大学公开课CS224W的学习笔记**
- **Learner** : shenhao
- **Lecture 3-4**

## 2.0 传统的图机器学习 VS  图表征学习

**传统的图机器学习**：

给定一个输入图，提取节点、链接和图级别的特征，学习出一个将特征映射到标签的模型(SVM、神经网络等)。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129205131840.png" style="zoom:33%;" />

一般来说，工程师会花大部分精力在特征工程上，来尽可能搞清楚如何能够更准确地描述特定的图，从而使得这个网络对于下游预测任务来说效果甚佳。

**图表征学习**：

图表示学习减少了每次都要进行特性工程的需要。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129205552047.png" style="zoom:33%;" />

**目标**：运用高效的任务独立型特征学习方式进行图机器学习。

如果我们在独立节点级别执行该操作，我们想要模型学会如果讲图上的一个节点 $u$ 映射到 $d$ 维空间。而这个 $d$ 维向量称为特征表示。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129210307525.png" style="zoom:33%;" />

**任务**：将节点映射到嵌入的空间中

嵌入向量往往有以下的特点

- 节点间嵌入的相似性也表示着节点在网络中的相似性，比如在图中相邻的节点它们的嵌入向量也是相似的。
- 编码网络信息。
- 可以被用于许多下游预测

## 2.1 节点嵌入 Node Embedding

**图表示学习**的目标是提供一个统一的图特征提取方式，对于不同的图机器学习任务都可以使用，而节点嵌入就是指将图中的节点映射到嵌入空间中，用一个稠密的向量来表示不同的节点，而向量的相似度又决定了节点在图中的相似度，也就是说将整个网络进行了编码。

### 2.1.1 编码器和解码器

节点嵌入的目标就是对节点进行编码并映射到嵌入空间中，使得两个节点在嵌入空间中的相似度近似于节点在图中的相似度，而**相似度和嵌入向量的形式都是需要定义**的

- 因此节点嵌入的两个关键的组件就是编码器和相似度函数。
- 解码器：输入两个嵌入向量，输出映射到相似度分数，在下式中解码器就是数量积。
- 相似度函数：指定向量空间中的关系如何映射到原始网络中的关系。输入为两个节点，输出相似度。

$$
\mathrm{similarity}(u,v)=z_u^Tz_v
$$



<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20210322102505885.png" style="zoom: 33%;" />



### 2.1.2 浅编码Shallow Encoder

最简单的编码器的形式就是一个简单的嵌入映射，即将节点通过矩阵运算直接转化成对应的嵌入向量，可以表示为：
$$
\mathrm{ENC}(v)=\bold Zv
$$

- 其中Z就是一个$d\times |V|$维度的矩阵（$V$为节点数），存储了每个节点的d维嵌入向量，而v就是一个0-1向量，除了对应的节点那一列是1以外都是0
- 我们需要学习的就是Z矩阵，这种编码器下面每个节点都映射到一个单独的嵌入向量中。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129212319616.png" style="zoom: 33%;" />

### 2.1.3 如何定义节点相似性

上述流程中的关键在于是如何定义节点相似度！

两个节点具有相似的嵌入的几种界定：

- 是否被一条边相连
- 是否有很多共同的邻近节点
- 是否处于网络中相似部分，或者它们周围的网络看起来很相似。

下章节我们将依据随机游走来界定节点的相似度，学习如何使用随机游走相似度度量优化节点嵌入！！

- 这是个无监督/自监督学习节点嵌入的方法
  - 没有用节点标签
  - 没有用节点特征
  - 目标是直接估算一组节点坐标来保留网络结构的一些层面。
- 这些嵌入向量与任务无关
  - 它们不是为特定任务训练的，但可以用于任何任务

### 2.1.4 随机游走Random Walk

#### 2.1.4.1 什么是随机游走

随机游走是一种用来定义图节点相似度的方法，  $z_u$ 表示图节点 $u$ 的嵌入向量，而概率$P(v|z_u)$表示在节点 $u$ 的随机游走中遇到节点 $v$ 的概率。

随机游走的过程即每次**随机选择**当前节点的一个邻居并“走”到这个邻居的位置上不断重复的过程，这个过程中将产生一个随机的节点序列，称为图的随机游走。而用随机游走定义的相似度就是**u和v出现在同一个随机游走中的概率**。这种方式计算相似度需要以下几个步骤：

- 使用一定的决策策略 $R$ 来计算从 $u$ 出发的随机游走经过 $v$ 的概率
- 根据随机游走的结果优化嵌入函数并进行编码

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129213147102.png" style="zoom:50%;" />

#### 2.1.4.2 为什么需要随机游走

- **可表达性**：随机游走是一种灵活并且随机的相似度定义，并且包含了局部信息和更高阶的图中领域节点关系。
  - 如果一个随机游走很大概率是从节点 $u$ 和节点 $v$ 开始的，那么 $u$ 和 $v$ 有很大概率是十分相似的。
  - 它们可能有着相似的网络领域，彼此相距很近，它们之间可能有很多条路径等等。
- **高效性**：不需要在训练的过程中考虑所有的节点对，只需要考虑在随机游走中出现的节点对。
- 随机游走是一种**无监督的特征学习**。

#### 2.1.4.3 随机游走的优化

随机游走的目标是让学习到的嵌入向量中，相近的向量在图中更接近，对于一个节点 $u$ 可以定义它在某种选择策略 $R$ 下的随机游走中发现的邻居节点构成的集合是$N_R(u)$（领域就是随机游走中从节点 $u$ 开始访问的节点序列）。

对于一个给定的图$G=(V,E)$，我们的目标是学习出一个映射函数$f(u)=z_u$，根据极大似然估计，我们的目标函数可以确定为：
$$
\max_f\sum_{u\in V}\log P(N_R(u)|z_u)
$$
我们的目标是**学习节点到它们嵌入的映射函数 $Z$ ，最大化当给定 $u$ 时，出现在它的随机游走领域中的节点的log概率之和**。意味着我们想将同一随机游走的节点嵌入向量近些。

即对于给定的节点 $u$ ，我们希望通过随机游走中的表现来学习其特征的表示，而随机游走可以进行一系列的优化，包括：

- 进行一个较短的固定长度的随机游走
- 对于每个节点 $u$ 的邻居节点，允许邻居节点集合中出现重复的节点，出现的越多表明相似度越高，因此最大化上述目标函数可以等价于最小化下面的表达式：

$$
\mathcal L=\sum_{u\in V}\sum_{v\in N_R(u)}-\log (P(v|z_u))
$$

- 而概率$P(v|z_u)$可以用**sotfmax函数**来进行参数化，选用softmax函数的原因是因为指数运算避免了负概率的出现，并且使得不同节点的相似度区分变得更加明显

$$
P(v|z_u)=\frac{\exp(z_u^Tz_v)}{\sum_{n\in V}\exp(z_u^Tz_n)}
$$
<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211129214658113.png" style="zoom:33%;" />

总之，我们的优化目标是找到一个函数 $z$ ，最小化损失 $\mathcal{L}$ 。
$$
\mathcal{L}=\sum_{u \in V} \sum_{v \in N_{R}(u)}-\log \left(\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V} \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)}\right)
$$


但是用上述办法来计算目标函数的话复杂度是非常高的，为$\mathrm{O}\left(|\mathrm{V}|^{2}\right)$！！！

可以**采用负采样的方式来近似计算损失函数**，这里用到了sigmoid函数来近似计算：
$$
\log (\frac{\exp(z_u^Tz_v)}{\sum_{n\in V}\exp(z_u^Tz_n)})\approx\log (\sigma(z_u^Tz_v))-\sum_{i=1}^k\log(\sigma(z_u^Tz_{n_i}))
$$

- 这种近似方法不计算全部节点而是只采样了 $K$ 个随机的负样本，并且用sigmoid函数来近似指数运算，这里的 $k$ 个negative nodes按照其度数成正比的概率进行选取。

-  $k$ 一般取 5-20

  - 更高的 $k$ 给出更稳健的估计。
  -  $k$ 越高，对应更多的采样，对负例子选择的权重更大，从而对负面事件的偏见就越高。



在得到了目标函数的近似形式之后，我们可以采用**随机梯度下降法**来对目标函数进行优化，定义
$$
\mathcal L^{(u)}=\sum_{v\in N_R(u)}-\log (P(v|z_u))
$$

> 随机梯度下降:与其在所有示例(所有节点)中评估梯度，不如针对每个单独的训练示例(给定的节点)进行评估.

  - 用随机数初始化节点嵌入向量 $z_i$

  - 对于一个节点 $i$ ，和其领域中所有的节点 $j$ ，计算其导数 $\frac{\partial\mathcal L^{(i)}}{\partial z_j}$

  - 更新每一个向量 $j$ 直到收敛
    $$
    z_j=z_j-\eta\frac{\partial\mathcal L^{(i)}}{\partial z_j}
    $$

#### 2.1.4.4 随机游走的策略

到目前为止，我们已经描述了如何在给定随机游走策略 $R$ 的情况下优化嵌入向量。

那么随机游走的策略有哪些呢？

- 最简单的想法：我们进行从每个节点开始的，一定长度的不带权重的随机游走。（eg：[Deep walk](https://arxiv.org/pdf/1403.6652.pdf)）
  - 问题是，这种相似性的概念太局限了。
- 进一步的想法：用**更加丰富的随机游走**，让随机游走更有表现力，以便可以调整更多的嵌入向量。（eg：[node2vec](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf)）

### 2.1.5 node2vec

#### 2.1.5.1 概述

- 目标：在特征空间中嵌入具有相似网络邻域的节点
- 关键点：灵活的网络邻域概念：指向更加丰富的节点嵌入

- 简单的随机游走的延伸：用**二阶随机游走**来产生网络领域 $N_R(u)$ 。

[Deep walk](https://arxiv.org/pdf/1403.6652.pdf) 和 [node2vec](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) 的唯一区别是：这些相邻节点集是如何被界定的，以及随机游走是如何被界定的。

node2vec是一种更加灵活的，可以在图的局部和全局信息之间进行权衡的加权随机游走。

- 局部信息：广度优先搜索 BFS ，注重节点附近的网络，微观。
- 全局信息：深度优先搜索 DFS ，尽可能多地探索网络，宏观。
- 可以用不同的方式对网络进行探索。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130001632546.png" style="zoom: 33%;" />



#### 2.1.5.2 BFS和DFS之间插值

我们将使用加权的一定长度的随机游走策略 $R$ ，给定节点 $u$ ，生成领域 $N_R(u)$ 。
以下为两个超参数：

- 折回参数(Return parameter) p
  - 表示随机游走往回走到之前的节点的可能性
- 深浅参数(In-out parameter) q
  - 表示用 DFS 深入探索网络节点（向外移动）和用 BFS 潜在探索周边网络节点（向内驻留）的比率

这种类型的随机游走策略就叫做二阶随机游走，**意味着它记得自己从哪里来**。

比如在下面这个例子中，随机游走从节点 $S_1$ 到节点 $W$ 。现在在节点 $W$ 上，随机游走需要决定走哪个方向，有三个选择：

- 返回 $S_1$
- 走到和它之前来的地方距离相同的节点，一步远的 $S_2$
- 走到更远的节点，两步远的 $S_3$

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130002831848.png" style="zoom:33%;" />

我们对此进行参数化：

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130003739366.png" style="zoom:33%;" />

- $p$，$q$ 模型的转移概率
- $1/p$，$1/q$，1 是非归一化的概率
- 我们将非归一化转换概率分布归一化求和，再进行采样，选择概率最大的作为随机游走的下一个节点。
- 倾向于 BFS 的游走： $p$ 值小
- 倾向于 DFS 的游走： $q$ 值小

每次走到新的节点的时候计算邻近节点的权重，选择权重最高(这里的权重其实也就是代表了走到这个节点的概率，当然是没有标准化的概率)的节点作为游走的下一个目的地。

#### 2.1.5.3 node2vec算法框架

- 计算随机游走的概率分布
- 模拟从每个节点 $u$ 开始， $r$ 条长度为 $l$ 的有偏(加权)随机游走
- 用负采样近似目标函数，并使用随机梯度下降法来优化node2vec。

#### 2.1.5.4 总结

**优势：**

- **线性**时间的复杂度
  - 因为每个节点都有一组固定的随机游走，所以它关于图的大小是线性的。
- 上述算法框架中的三个步骤是可以**并行**的

**劣势：**

- 需要**单独学习每个节点的嵌入向量**
  - 因此对于更大的网络，我们需要学习更大或者更多的嵌入。

## 2.2 图嵌入

> 匿名的随机游走

目标：将一个子图或者整个图嵌入到嵌入空间 $z$ 中。

任务：

- 分子分类来预测哪些分子是有毒的，哪些是无毒的。
- 图形异常检测

总之，图嵌入是将整张图或者子图映射到嵌入空间中，用一个向量来表示

### 2.2.1 几种简单的approach

**方法1**

将图嵌入等价于图中所有节点的嵌入向量之和（或者是平均）
$$
Z_G=\sum_{v\in G}z_v
$$

**方法2**

在途中引入一个虚拟节点代表整个图(子图)并进行嵌入

![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130152608923.png)

如果我们想要嵌入整张图，那么这个虚拟节点 $S$ 会将该网络中的所有其他节点连接起来。并用 Deepwalk 或者 node2vec 进行计算，从而确定虚拟节点的嵌入。并且把整个图的嵌入表示为红色节点的嵌入 $z_S$ 。

### 2.2.2 匿名游走嵌入

#### 2.2.2.1 定义

[匿名游走嵌入](https://arxiv.org/pdf/1805.11921.pdf)：匿名游走重点的状态对应于在游走中访问给定节点时的第一次索引

- 匿名：不知道被访问节点的身份

![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130153622492.png)

#### 2.2.2.2 游走增长的速度

匿名游走随长度呈指数增长

![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130153954049.png)

长度为3的匿名游走有5种：

$w_1=111, w_2=112, w_3= 121, w_4= 122, w_5= 123$

#### 2.2.2.3 匿名游走的简单使用

简单地模拟长度为 l 的匿名游走，并记录各种类型匿名游走的次数，然后将图以这些游走的概率分布的形式表示出来。

**例如**：

- 选择 $l=3$ ，那么可以得到5种匿名游走。
  - $w_1=111, w_2=112, w_3= 121, w_4= 122, w_5= 123$
  - 通过长度的设定，可以得到图嵌入的维度
- 我们可以将一个图用五维向量 $Z_G$ 表示。
-  $Z_G[i]$ 等于在图 $G$ 中出现 $i$ 类型的匿名行走的概率或者次数的分数

#### 2.2.2.4 采样次数

匿名游走的采样次数 $m$ 的公式（以便估算的出现频率或者概率是准确的）：
$$
m=\left\lceil\frac{2}{\varepsilon^{2}}\left(\log \left(2^{\eta}-2\right)-\log (\delta)\right)\right\rceil
$$

> - 可以通过两个参数， $\varepsilon$ 和 $\delta$ ，来量化精确度，误差不超过 $\varepsilon$ ，概率小于 $\delta$ 。
> - 比如， $l=7,\varepsilon=0.1,\delta=0.01$ ，得到 $\eta=877,m=122,500$ 。
> -  $\eta=877$ 是匿名游走的个数，由 $l$ 决定.

#### 2.2.2.5 改进

与其简单地用每步游走出现的次数来表示它，我们可以学习匿名游走 $W_i$ 的一个嵌入 $Z_i$ 。

- 我们也可以学习我们的图嵌入 $Z_G$ 以及匿名游走的嵌入 $z_i$ 。

#### 2.2.2.6 如何嵌入匿名游走呢？

输入：图 $G$ 
输出：$Z_G$ 向量 

- 从节点 1 开始，采样匿名游走

  ![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130163234064.png)

- 学习预测在 $\Delta-size$ 窗口中同时发生的游走 (e.g. 如果 $\Delta=2$ ，给定 $w_1,w_2$ ，预测 $w_3$ ) 

- 目标函数：$T$ 是匿名游走的总个数
  $$
  \max _{\mathbf{z}_{G}} \sum_{t=\Delta+1}^{T} \log P\left(w_{t} \mid w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{G}\right)
  $$

 **思路** ：

- 我们从每个节点 $u$ 开始进行长度为 $l$ 的 T $个$不同的随机游走。

- 邻域的概念变为一组匿名的游走
  $$
  N_{R}(u)=\left\{w_{1}^{u}, w_{2}^{u} \ldots w_{T}^{u}\right\}
  $$

- 我们想要学习预测在 $\Delta$ 大小窗口中共同出现的游走。

- 估算匿名游走 $w_i$ 的嵌入 $z_i$ 。

- 我们就可以在匿名游走的抽样中，预测下一次匿名游走是什么。

  ![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130164111636.png)

- **优化目标**：
  $$
  \max _{z_{i}, z_{G}} \frac{1}{T} \sum_{t=\Delta}^{T} \log P\left(w_{t} \mid\left\{w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{G}\right\}\right)
  $$

  - $P\left(w_{t} \mid\left\{w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{\boldsymbol{G}}\right\}\right)=\frac{\exp \left(y\left(w_{t}\right)\right)}{\sum_{i=1}^{\eta} \exp \left(y\left(w_{i}\right)\right)}，\eta$ 是匿名游走的所有可能性个数，需要负采样。
  - $y\left(w_{t}\right)=b+U \cdot\left(\operatorname{cat}\left(\frac{1}{\Delta} \sum_{i=1}^{\Delta} \mathbf{z}_{i}, \mathbf{z}_{G}\right)\right)$
    - $\operatorname{cat}\left(\frac{1}{\Delta} \sum_{i=1}^{\Delta}{z}_{i},{z}_{G}\right)$ 表示窗口中匿名嵌入 $z_i$ 的平均值，与图嵌入 $z_G$ 相连接。
    - $b \in \mathbb{R}, U \in \mathbb{R}^{D}$ 是学习参数，代表线性层。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211130164955520.png" style="zoom: 50%;" />

## 2.3 链接分析：PageRank算法

这一节的内容主要从矩阵的角度来进行图的分析和学习。我们将图视为矩阵后：

- 能够通过随机游走定义节点重要性(PageRank算法的思想)
- 能够通过矩阵分解(MF)获得节点嵌入
- 能够查看其他节点嵌入(例如Node2Vec)（也是基于矩阵分解的形式）

因此可以得到随机游走、矩阵分解和节点嵌入是密切相关的！！

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201152650352.png" style="zoom: 33%;" />

### 2.3.1 PageRank算法简介

我们可以将互联网看成一张巨大的图，里面的网页就是图中的一个个节点，而网页可以通过超链接可以跳转到别的网页，称为链接link，可以把这种关系作为图中的有向边，因此互联网中的网页构成一张**大而稀疏的有向图**，可以用一个邻接矩阵来表示。类似的结构有论文引用图等等。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201153101440.png" style="zoom:33%;" />

但是图中每个节点并不都是同等重要的，可以通过一系列链接分析的方法来分析出不同节点的重要性，一般认为一个网站如果有很多链接，那么它往往就比较重要。而出链接和入链接又是两种不同的链接，又应该有不同的考虑。PageRank算法认为**被重要的网页指向的网页**也往往更重要。这也是个递归的问题。

### 2.3.2 PageRank算法模型

#### 2.3.2.1 节点的权重

给定链接关系 $i\rightarrow j$ ，我们可以在网络图中定义 $d_i$ 是一个节点的出度，而节点的权重（重要性） $r_j$ 就可以表示为：
$$
r_j=\sum_{i\rightarrow j}\frac{r_i}{d_i}
$$
<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201154733249.png" style="zoom: 50%;" />

我们可以将这种表示形式矩阵化，用一个矩阵M来表示各个节点之间的权重关系，那么根据上面的定义可以得知：
$$
M_{ij}=\frac{1}{d_j}
$$
这个矩阵M每一列的和都是1，我们可以用一个向量 $r$ 来表示每个网页的重要程度，那么就有：
$$
r=Mr
$$

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201155431587.png" style="zoom:33%;" />

#### 2.3.2.2 图中的随机游走

在任何一个时间 $t$ 时假设访问到了网页 $i$ 中，下一个时刻 $t+1$ 则访问 $i$ 指向的其中一个网页 $j$ ，这样就构成了一次随机游走，用 $p(t)$ 向量来表示 $t$ 时刻每个网页被访问到的概率，那么这样一来就有：
$$
p(t+1)=Mp(t)
$$
我们发现矩阵的权重向量 $r$ 也可以表示随机游走的平稳分布情况，进一步我们发现 $r$ 其实就是矩阵 $M$ 的特征值为1的**特征向量**，因此PageRank实际上就是矩阵 $M$ 最大的的特征向量，我们一般用幂法可以得到矩阵 $M$ 的最大特征向量。

### 2.3.3 如何求解PageRank（Power iteration）

- Power iteration

  - 可以给pagerank赋予一定的初始值，然后通过公式$1·r=M·r$不断迭代直到 $r$ 的变化小于一定的阈值之后$\left|\boldsymbol{r}^{(\boldsymbol{t}+\mathbf{1})}-\boldsymbol{r}^{(t)}\right|_{1}<\varepsilon$才结束，得到最终的pagerank的结果，这种方法也就是幂法，求出的结果实际上是 $M$ 的特征之中范数最大的那一个。

  - 大约50次迭代就足以估计极限解。

  - 例子：

    <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201161551626.png" style="zoom:33%;" />

- 问题在于：

  - 有一些页面时dead end，没有跳转到其他网页的出链接，这可能会导致“泄漏”

    <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201161849649.png" style="zoom:33%;" />

  - Spider-Trap问题：所有的外部链接都在一个组内，即随机游走会陷入一个循环中

    <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201161800047.png" style="zoom:33%;" />

  - 这些情况都会导致上述计算方法最后不收敛，因此要想办法解决这个问题

- 解决方法：

  - 对于dead end问题，可以重新调整矩阵M中的内容：

    - 原本 $m$ 节点没有到任何节点的出链接，因此为0，现在让它到所有节点的概率相同

    <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20210323203140803.png" style="zoom:33%;" />
  
  - 对于Spider-Trap问题，可以在每次做选择的过程中以一定的概率跳转到随机的网页中去，这样就可以从循环中跳出来
  
    - $\beta$ 继续随机游走的概率
    - $1-\beta$ 跳转到随机的节点的概率
    -  $N$ 所有节点的个数

$$
r_j=\beta\sum_{i\rightarrow j}\frac{r_i}{d_i}+(1-\beta)\frac 1N
$$

Google矩阵上面的spider-trap问题的迭代形式
$$
G=\beta M+(1-\beta)[\frac{1}n]_{n\times n}
$$

**例子**：

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201162700934.png" style="zoom:33%;" />

### 2.3.4 PageRank的变种

#### 2.3.4.1 传统的PageRank

- 传送集 S 是网络的所有节点，都具有相等的概率
- $S = [0.1,0.1,0.1, 0.1,0.1,0.1, 0.1,0.1,0.1, 0.1] $

#### 2.3.4.2 个性化的PageRank

考虑到了不同用户之间的浏览偏好往往不同，进行个性化推荐

- 通过随机游走定义节点之间的相似性
- 传送集 S 是节点的子集，**特定节点或节点集**，eg：用户感兴趣的产品，产品对应的用户等等
- $S = [0.1,0,0,0.2, 0, 0,0.5,0,0, 0.2] $

#### 2.3.4.3 带重启的随机游走

- 通过随机游走定义节点之间的相似性

- 传送集 S 是一个节点（也就是起始节点）
- $S = [0,0, 0,0, 1, 0, 0, 0, 0, 0,0]$

### 2.3.5 嵌入和矩阵分解的联系

**基于随机游走的节点嵌入可以表示为矩阵分解**！！！

#### 2.3.5.1 最简单的节点相似性与矩阵分解的关系

我们定义一个最简单的节点相似性：如果节点 $u$ 和 $v$ 通过边连接，则它们类似。

这意味着：$\mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}=A_{u, v}$，这是图邻接矩阵 $A$ 的 $(u,v)$ 项，因此可以进行矩阵分解：$A=\boldsymbol{Z}^{T} \boldsymbol{Z}$

![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201173226933.png)

- 嵌入维 $d$ ( $Z$ 中的行数) 远小于节点数 $n$ 。
- 精确的因子分解$A=\boldsymbol{Z}^{T} \boldsymbol{Z}$通常是不可能的。
- 然而，我们可以大致了解 $Z$ 。
- 优化目标：$\min _{\mathbf{Z}}\left\|\mathrm{A}-\boldsymbol{Z}^{T} \boldsymbol{Z}\right\|_{2}$

结论：由边的连通性定义的节点相似度内积解码器等价于 $A$ 的矩阵分解。

#### 2.3.5.2 基于随机游走的节点相似性与矩阵分解的关系

 DeepWalk 和 node2vec 使用了基于随机游走的更复杂的节点相似度定义。

DeepWalk等价于以下复杂矩阵表达式的矩阵分解：
$$
\log \left(\operatorname{vol}(G)\left(\frac{1}{T} \sum_{r=1}^{T}\left(D^{-1} A\right)^{r}\right) D^{-1}\right)-\log b
$$

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201173843676.png" style="zoom:50%;" />

- T 是上下文窗口的大小，即领域的大小，$T=\left|N_{R}(u)\right|$。
- $\operatorname{vol}(G)$ 是图的容量，所有相邻节点计算值的累加式，$\operatorname{vol}(G)=\sum_{i} \sum_{j} A_{i, j}$。
-  $D$ 是对角矩阵。
-  $b$ 是负采样的样本数。
-  $r$ 是归一化邻接矩阵的幂。

> Node2vec也可以被表述为一个矩阵分解(尽管是一个更复杂的矩阵)。

### 2.3.6 局限

基于矩阵分解和随机游动的节点嵌入的局限性

- 无法获得不在训练集中的节点的嵌入

  - 对于 DeepWalk 和 node2vec 算法，意味着每加入一个新的节点，需要重新计算所有节点的嵌入。

- 无法捕捉结构相似性

  ![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201174818655.png)

  - 节点1和11在结构上相似——是一个三角形的一部分，度数为2。
  - 然而，它们有着非常不同的节点嵌入值。
    - 因为随机游走不太可能从节点1到达节点11
  - 对于 DeepWalk 和 node2vec 算法而言无法捕捉结构相似性。

- 不能利用节点、边和图形的特征

  ![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211201175200916.png)

### 2.3.7 解决方法

- 深度表征学习
- 图神经网络

### 2.3.8 总结

**PageRank**

- 度量了图中节点的重要性
- 可以通过幂迭代法有效计算邻接矩阵

**个性化PageRank(PPR)**

- 度量了节点相对于特定节点或节点集的重要性
- 可以用随机游走高效计算

**重点**：**基于随机游走的节点嵌入**可以表示为**矩阵分解**，而**将图表示为矩阵**在上述算法中起着**关键作用**！

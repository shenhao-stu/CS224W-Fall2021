# CS224W：图机器学习3

## 消息传递和节点分类

这一节内容主要关注：对于一个给定的网络结构，网络中的部分节点已经做好了标注，那么我们如何对图中的其他节点进行标注呢？

- 举例：已知网络中有一些诈骗网页，还有一些可信网页，那么如何根据网页间的超链接，进一步找到其他诈骗或者可信的网页节点？

- 对于训练数据中一部分有标签，剩下的没标签，这类任务被称为半监督的节点分类问题。

  > 这样的半监督学习经常被用在：
  >
  > - 文本分类
  > - 语音标记
  > - 连接预测
  > - 光学字符识别
  > - 图像和3D分割
  > - 节点异常检测

之前已经介绍了**节点嵌入**的方法可以识别相似的节点以及处理节点分类问题，而这一节主要介绍的方法是**消息传递**机制。

### 概述

一般我们认为网络中的关系是具有相关性的，也就是说相似的节点会互相连接，并且相近的节点可能属于相同的类型。即同质性和影响力决定了节点间会存在关联。

- **同质性**：人以类聚，物以群分 | 相同或者类似的节点会和其他的节点相互连接。

  - 举例：一个在线社交网络，以人为节点，朋友关系为边，通过人们的兴趣将节点分为多类。
    从图中可以看出，各种颜色都分别聚在一起，亦即有相同兴趣的人们更有聚集在一起的倾向。

  <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217155639360.png" alt="image-20211217155639360" style="zoom:50%;" />

  

- **影响力**：近朱者赤，近墨者黑 | 给定节点对自身标签的信念会收到周围节点标签的影响。

类似于 PageRank 中将节点的 vote 通过链接传递到下一节点，但是在这里我们更新的**不再是重要性的分数**，而是对**节点标签预测的概率**。

本章内容将主要介绍三种半监督图学习的技术：

- 关系分类 Relational classification
- 可迭代分类 Iterative classification
- 信任传播 Belief propagation

首先我们先思考一下解决分类问题思路是什么？

### 半监督学习

**给定**：具体的图，一些带标签的节点

**目标**：对其余没有标签的节点进行分类

**主要假设**：该网络具有**同质性**（相似节点会在网络中更加靠近，或者直接相连）

**任务**：

- $A$： n×n 的邻接矩阵
- $Y=\{0,1\}^n$是标签向量
  - $Y_v=1$ 属于第一类
  - $Y_v=0$ 属于第二类
  - 一些没有标签的节点

![image-20211217160058996](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217160058996.png)

#### 基本的逻辑

我们已知**相似节点会在网络中更加靠近，或者直接相连**。

因此根据关联推定：如果我与具有标签 $X$ 的节点相连，那么我也很可能具有标签 $X$ 。

而对于图而言，预测一个节点 $v$ 的标签，我们需要：

-  $v$ 的特征
-  $v$ 邻居的标签
-  $v$ 邻居的特征

### 关系分类

关系分类的基本想法：

- 节点 $v$ 的类概率 $Y_v$ 是一个节点附近节点的类别概率的**加权平均**(这里我们只考虑二分类问题)
- 对于已经有标记的节点 $v$ ，其 $Y_v$ 可以**直接设置成其真实类别**
- 对于没有标记的节点可以将其初始化为 $Y_v=0.5$ 
- 然后以随机的顺序，对所有的节点进行类似于加权平均的更新，直到收敛或者达到最大迭代次数上限。

更新的公式：
$$
P(Y_v=c)=\frac{1}{\sum_{(v,u)\in E}A_{v,u}}\sum_{(v,u)\in E}A_{v,u}P(Y_v=c)
$$

- 邻接矩阵 $A_{v,u}$ 表示边的权重
- 分母是节点 $v$ 的度数或入度
- $P(Y_v=c)$ 是节点 $v$ 标签为 $c$ 的概率

问题在于：收敛是没有保证的，并且模型没有用到节点的特征信息

#### 举例

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162238659.png" style="zoom:33%;" />

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162304589.png" style="zoom:33%;" />

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162718100.png" style="zoom:33%;" />

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162851520.png" style="zoom:33%;" />

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162911663.png" style="zoom:33%;" />

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217162938291.png" style="zoom:33%;" />

在第四次迭代后，所有的分数都收敛稳定了。我们可以预测出：

- 节点4，5，8，9属于类别1
- 节点3属于类别2

### 迭代分类

相比于关系分类（没有用到节点的属性），迭代分类会使用节点 $v$ 的一系列特征构成的特征向量 $f_v$ 和邻近节点的标签 $z_v$ 。

**输入**：图

-  $f_v$ ：节点 $v$ 的一系列特征构成的特征向量
- $Y_v$ ：节点 $v$ 的标签

**任务**：预测没有标签节点的标签。

**两个分类器**：

- $\phi_1(f_v)$ ：通过节点的特征来预测节点的标签 | **基础分类器**（base classifier）。

- $\phi_2(f_v,z_v)$ ：通过特征向量和邻近节点的标签的汇总来共同预测节点的标签 | **关系分类器**（relational classifier）

训练两个分类器只是第一阶段，第二阶段需要进行迭代训练直到结果收敛或者训练结束，但是结果是否收敛依然没有保证。

#### 如何计算 $z_v$

 $z_v$ 是捕捉节点 $v$ 周围节点标签的向量，有以下几种表示：

-  $N_v$ 中每个标签的数量（或分数）的直方图
-  $N_v$ 中最常出现的标签
-  $N_v$ 中不同标签的个数

#### 迭代分类器的结构

1. 第一步：基于节点特征建立分类器

  - 在训练集上训练如下分类器 (可以用线性分类器、神经网络等算法)：

  - $\phi_{1}\left(f_{v}\right)$ 基于 $f_{v}$ 预测 $Y_{v}$
  - $\phi_{2}\left(f_{v}, z_{v}\right)$ 基于 $f_{v}$ 和 $z_{v}$ （ $v$ 邻居标签的summary）预测 $Y_{v}$

2. 第二步：迭代至收敛

  - 在测试集上，用 $\phi_{1}$ 预测标签，根据 $\phi_{1}$ 计算出的标签计算 $z_{v}$ 并用 $\phi_{2}$ 预测标签

  - 对每个节点**重复迭代**计算 $z_{v}$ 、用 $\phi_{2}$ 预测标签这个过程直至收敛或到达最大迭代次数 $(10,50,100,\dots)$

> 注意：模型不一定能收敛

#### 举例：网页分类

**节点**是网页，**链接**是超链接，**链接有向**（通过超链接指向另一个页面），**节点特征**简化设置为2维二元向量。

**任务**：预测网页主题

- 首先训练一个基于二维特征向量的分类器，用邻近节点（区分出和入节点）的特征向量作为$z_v$
- 然后按照步骤训练出两个分类器，然后用测试集来测试分类器的效果，并对关系特征$z_v$和标签进行更新
- 进行迭代更新，直到收敛或者达到次数上限

**具体流程**

1. 训练一个基于节点特征训练分类器（$\phi_1$）

​		可以假设分类器以特征第一个元素作为分类标准，于是对中间节点分类错误。![](https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217181612079.png)

2. 根据 $\phi_{1}$ 得到的结果，计算 $z_{v}$ 

   此处设置 $z_{v}$ 为[2,2]的张量

   -  $I$ ：入度的邻居标签信息向量，维度为2。
     - 第一维度表示标签为0的邻居节点的个数
     - 第二维度表示标签为1的邻居节点的个数
   - $O$​：出度的邻居标签信息向量，维度为2。
     - 第一维度表示标签为0的邻居节点的个数
     - 第二维度表示标签为1的邻居节点的个数
   -  $I_0 = 1$ 表示至少有一个指向该节点的邻居节点标签为1

   在这一步应用了网络结构信息。

   <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217182431504.png" style="zoom:33%;" />

3. 具体流程

   - 第一步：在训练集上训练 $\phi_1$ 和 $\phi_2$

     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217182636512.png" style="zoom:33%;" />
   
   - 第二步：在测试集上用 $\phi_1$ 预测标签
   
     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217182812913.png" style="zoom:33%;" />
   
   - 第三步：循环迭代——更新相关特征 $z_v$
   
     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217183155948.png" style="zoom:33%;" />
   
   - 第四步：循环迭代——使用 $\phi_2$ 更新标签 $Y_v$
   
     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217183252521.png" style="zoom:33%;" />
   
   - 第五步：迭代直到收敛
   
     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217183422524.png" style="zoom:33%;" />
   
   - 第六步：停止迭代
   
     <img src="https://gitee.com/shenhao-stu/picgo/raw/master/DataWhale/image-20211217183459064.png" style="zoom:33%;" />

#### 小结

关系分类

- 基于给定节点的邻居标签概率，迭代更新节点属于一个标签类的概率。

迭代分类

- 改进集合分类来处理属性/特征信息。
- 根据节点 $v$ 的特征以及邻居的标签对其进行分类




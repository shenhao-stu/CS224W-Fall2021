# CS224W 图机器学习1

- **斯坦福大学公开课CS224W的学习笔记**
- **Learner** : shenhao
- **Lecture 1-2**

## 图机器学习导论

### 图机器学习研究的问题

- 图结构和网络结构是非常常见的数据结构，常见的事件图、计算机网络、事务网、社交网络和人际关系网等等都是图状结构。
  
  ![](https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128233709729.png)
  
  - **无标度网络**经常被称为**自然图** Natural Graphs
  
    > 无标度网络：大多数“普通”节点拥有很少的连接，而少数“热门”节点拥有极其多的连接。各节点的度服从幂律分布。
  
  - 复杂领域具有丰富的关系结构，可以用关系图表示
  
  - 图是对于一些关系的表示，可以表示信息和知识的联系，**软件也可以表示为一张图**
  
  - 很多时候网络和图的区别是非常模糊的，通常可以将它们混为一谈。
  
  - 本课程要研究的也就是如何利用图中的关系来进行更好的预测.
  
- 为什么图深度学习相比CV，NLP更难？

  - 相比于图像和文本，图结构的大小比较随意且拓扑结构复杂（没有像网格那样的空间局部性）。

    <img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128150735108.png" style="zoom:25%;" />

  - 没有固定的节点顺序或者参考点。

  - 通常是动态的，具有多模态特征。

- 图学习的任务可以有 Node level ， Edge level ， Community(subgraph) level 和 Graph level 多个层次。
  <img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128150318590.png" style="zoom: 25%;" />

- 传统的图机器学习任务有：
  - 节点分类：预测节点的属性，比如分类在线用户或者商品。
  - 链接预测：预测两个节点之间是否缺少了链接，比如知识图谱补全
  - 图分类：比如化学中的分子性质预测
  - 聚类：检测节点是否构成一个类别，比如社交圈检测
  - 图生成：用以药物发现以产生新的分子结构
  - 图进化：物理现象的仿真模拟
  
- 图学习应用：

  - 节点级别任务：金融诈骗检测（典型的节点分类）、自动驾驶中的3D点云目标检测
  - 边级别任务：推荐系统（典型的边预测）
  - 图级别任务：气味识别（典型的图分类）、发现“宇宙”

  <img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128162738088.png" style="zoom:50%;" />

### 图的表示方式

#### 图的组成

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128153543488.png" style="zoom: 25%;" />

常见的表示方式使用节点集合N和边集合E构成一个图结构$G(N, E)$ 

#### 图的分类

- 图可以分为**有向图**和**无向图**，区别就在于边是否具有方向性，节点的度（node degree）表示包含这个节点的边的数量，而在有向图中又可以具体分为出度和入度。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128154159403.png" style="zoom: 33%;" />

- **二分图**是一种特殊的图结构，可以将图中的**节点**分成两个不相交的集合U和V，使得图中的任意一条边的两个顶点分别属于U和V
  - 作者和文章
  - 演员和电影
  - 消费者和电影
  - 菜肴和原材料
- **“折叠”网络**：由二部图建立 U or V 节点集合的图。
  - 作者合作网络
  - 电影共同评分网络
- **图的其他类型**：
    - **无权图 VS 加权图**
        <img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128155923388.png" style="zoom:25%;" />
    - **自循环、多边图**
        <img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128160212169.png" style="zoom:25%;" />

#### 图的表示

- 图可以用**邻接矩阵**来表示，其中无向图的邻接矩阵是一个对称矩阵，通常邻接矩阵都比较稀疏，网络结构通常都是比较稀疏的图。
- 图可以用**边列表**表示，存储有边连接的每一对节点的 ID。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128161610487.png" style="zoom:25%;" />

- 图可以用**邻接列表**来表示，这种表示方式只保留了有边的节点的情况，而舍去了稀疏的大部分内容，节约了存储空间。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128155759277.png" style="zoom: 25%;" />



#### 补充概念

**连通图**

- 强连通有向图：任意两个节点U和V之间存在U到V以及V到U的路径
- 弱连通有向图：任意两个节点U和V之间存在U到V或者V到U的路径
- 强连通分量：有向图中的强连通子图

**同构图与异构图**

- **同构图**：
  - 两个图G和H是同构图（isomorphic graphs），能够通过重新标记图G的顶点而产生图H。
  - 如果G和H同构，那么它们的阶是相同的，它们大小是相同的，它们个顶点的度数也对应相同。
- **异构图**：是一个与同构图相对应的新概念。

传统同构图（Homogeneous Graph）数据中只存在一种节点和边，因此在构建图神经网络时所有节点共享同样的模型参数并且拥有同样维度的特征空间。而异构图（Heterogeneous Graph）中可以存在不只一种节点和边，因此允许不同类型的节点拥有不同维度的特征或属性。

## 传统的图机器学习方法

> 节点的特征：
>
> - 结构特征：描述图网络拓扑结构的特征（特定节点在网络中的位置）
> - 描述特征：描述节点的属性和内容的特征

**在图上使用有效的特征是实现良好模型性能的关键**。传统的机器学习通过手工提取特征来完成图机器学习中的node-level，link-level和graph-level的一些预测。

**目标**是对于给定的图 $G=(V, E)$ 学习出一个从 V 映射到 R 上面的函数。
Given: $G=(V, E)$
Learn a function: $f: V \rightarrow \mathbb{R}$

### 节点层面的机器学习任务

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128191510304.png" style="zoom: 50%;" />

- 图机器学习在节点层面的主要任务是对节点进行分类和学习图的结构特征，主要有：
  - 节点的度数
  - 节点的中心（node centrality）
  - 聚类系数
  - 图元（graphlets）

#### **node degree**

节点 $v$ 的度数 $k_v$ 为节点所拥有的边（相邻节点）。
但是节点度的这个特征**不做区分地对待每个相邻的节点**，因此在某种意义上，就没法对相同节点度的节点进行区分，即使它们可能位于网络中的不同部分。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128192436271.png" style="zoom:33%;" />

#### node centrality

节点中心**node centrality**是在一个图中表示出不同节点的**重要性**，而不是简单的出入度数。一个节点v如果与一些很重要的节点相邻那么这个节点也很重要，一个节点的centrality可以这样计算：
$$
c_v=\frac{1}{\lambda}\sum_{u\in N(v)}c_u
$$

问题在于上面的这个表达式其实是一种循环定义，需要用另一种方法来表示。

**特征向量中心 Eigenvector centrality**

我们可以用向量的知识来表示上面的centrality定义式：
$$
\lambda c=Ac
$$

- 其中A是图G的邻接矩阵，如果两个点uv相邻，那么$A_{uv}=1$否则就是0
- 我们发现这时候由每个点的centrality组成的向量C实际上就是矩阵A的特征向量，其中最大的特征向量总是为正并且唯一的，因此可以用A矩阵的最大特征值作为图G的centrality

**中间性中心度 Betweenness Centrality **

该标准认为一个节点如果处于很多通往其他节点的最短路径上，那么这个节点就比较重要，因此定义：
$$
c_v=\sum_{s\not=t\not=v}\frac{N_{svt}}{N_{sv}}
$$

该标准下重要程度的计算方式就是对于图中的任意两个和 $v$ 不同的节点 $s$ ， $t $，计算其最短路径中包含 $v$ 的比例并进行求和。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128194035530.png" style="zoom: 33%;" />

**邻近中心度 Closeness Centrality **

该标准认为一个节点如果拥有比较小的到其他所有节点的最短路径长度，那么它就比较重要。
实质上就是，节点的位置越中心，通往其他节点的路径越短，节点越重要。用 $l$ 表示图中两个节点的最短路径长，那么：
$$
c_v=\frac 1 {\sum_{u\not=v}l_{uv}}
$$
<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128194400360.png" style="zoom: 33%;" />

#### 聚类系数

聚类系数可以衡量一个节点的邻居节点的连接情况，假设一个节点 $v$ 有 $k$ 个邻居，这些**k个邻居之间有n条边**，那么聚类系数可以写作：
$$
e_v=\frac{n}{C_k^2}=\frac{2n}{k(k-1)}
$$

> 聚类系数某种意义上也可以理解为记录了每个节点与其邻近节点构成的三角形的个数，即对于一个节点 $v$ ，如果它的两个邻居 $u$ 和 $w$ 之间有一条边，那么 $uvw $就构成了一个三角形，而这个三角形被统计到了聚类系数中去。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128200426780.png" style="zoom: 33%;" />

- 0意味着你的朋友，与你相联系的朋友之间彼此互不了解；
- 1意味着你所有朋友也是彼此的朋友；

#### Graphlets

观察聚类系数中的例子可以发现，聚类系数实际就是在数自我中心网络（例子中为给定节点周围的1级领域网络）中的三角形个数。

> **自我中心网络**：中心节点本身与它邻近节点之间组成的网络。
>
> **闭合三角形**：中心节点和两个邻近节点所形成的三元组图形。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128201031854.png" style="zoom:33%;" />

因此引出图元（Graphlets）这个概念所要描述的特征：**计算在给定节点的领域内预先指定的图的数量**。

Graphlets 是一种根连通的非同构子图，有如下几个衍生出的概念：

> G1 中有两个位置不同的节点1，2。G6 中有三个位置不同的节点9，10，11。G8 中所有的节点都是等价的。

![](https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20210321151423224.png)

Graphlet Degree Vector(GDV)：一个给定的图元以该给定节点为根出现的次数。基于Graphlets的节点特征，记录了每个节点所能接触到的graphlets的个数，和度数以及聚类系数不同，度数衡量了一个节点能接触到的边的数量，而聚类系数衡量了一个节点能接触到的“三角形”的个数，而GDV记录了以一个节点v为root的所有的graphlet。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20210321151615675.png" style="zoom:33%;" />

考虑 2-5 个节点的图元，我们可以得到：

- 可以用一个73维度的向量表示一个节点的特征，描述了节点的邻近区域的拓扑结构。
- 这个向量可以表示节点之间的相互连接情况，上至4个节点的距离。

GDV提供了一个节点的局部网络拓扑结构的衡量方式。

#### Tips

- 节点度：节点接触的边数
- 聚类系数：节点接触或参与的三角形的数量
- 图元度向量：节点参与的图元的数量

### 边层面的机器学习任务

图机器学习在边层面的主要任务是基于已经存在的连接来**预测新的连接**，因此关键的问题在于如何设计和表示**一对节点**的特征，一般有两种边预测的场景：

- 随机缺失了若干条边，要求将图补全
- 随时间会变化的边关系：在时许数据的场景下节点和节点之间的边关系可能会随时间发生变化，这种情况叫做evaluation，一般来说可以用如下步骤来解决时序预测问题：
  - 对每一个节点对计算一个score C，即一个既定的标准
  - 根据这个标准来对节点对进行排序
  - 预测排序中的top-n作为新的边，并进行验证

#### 基于距离的特征

- 用两个节点之间的最短路径距离来表示节点对的特征，但是这种方式没有考虑到邻近点的重合度

#### Local Neighborhood Overlap

​		用共同的邻近节点的个数作为一个节点对的特征，节点度越高的节点越可能与其他节点有邻居。对于一个节点v，用$N(v)$表示所有邻近节点构成的集合，那么有这样几种标准：

- 共同邻居：

$$
|N(u)\cap N(v)|
$$

- Jaccard系数

$$
\frac{| N(u)\cap N(v)|}{|N(u)\cup N(v)|}
$$

- Adamic-Adar index
  $$
  \sum_{u \in N\left(v_{1}\right) \cap N\left(v_{2}\right)} \frac{1}{\log \left(k_{u}\right)}
  $$

#### Global Neighborhood Overlap

​		局部的neighborhood overlap在两个节点没有共同邻居的时候始终为0，但这两个节点仍然有可能存在一定的关系，而Local Neighborhood Overlap这些关系的特征，因此就需要Global Neighborhood Overlap

- Katz index：记录两个节点之间所有的路径数，可以使用邻接矩阵来计算，假设临界矩阵为A，其中如果u和v相邻那么$A_{uv}=1$，这样一来$A_{uv}^{k}$就可以表示是否有连接uv的长度为K的路径，因此可以用下面的共识来计算这一特征量：

$$
S_{uv}=\sum_{l=1}^{\infin}\beta ^lA_{uv}^l
$$

- 而Katz index矩阵可以用如下方式来计算：

$$
S=\sum_{i=1}^{\infin}\beta^iA^i=(I-\beta A)^{-1}-I
$$



### 图层面的特征和图kernel

目标是提取整张图整体的特征，而传统机器学习中经常使用kernel来进行特征的提取，图也可以使用kernel来进行特征的提取，其核心是一个图特征向量$\phi(G)$ 

#### BoW

图kerbel的关键想法是**基于Bag-of-Word(BoW)**BoW原本是指使用单词出现的次数来作为文档的特征，而将其扩展到图上就可以将节点作为“单词”，即可以计算图中各种不同度数的节点的数量，并将结果作为一个多维的向量输出，向量的每一个维度d的值代表了图中度数为d的节点总数。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128212121184.png" style="zoom: 33%;" />

#### Graphlet Kernel

Graphlet Kernel 可以计算图中不同的graphlet的个数来表示图的特征，但是这里的graphlet和节点层面的不太一样，不需要完全连接，也没有根的概念

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20210321170512352.png" style="zoom:33%;" />

- 对于一个给定的图 $G$，用一个graphlet表来定义graphlet计数向量$f_G\in\mathbb R^{n_k}$ ，$f$ 定义为我们预设图中出现的给定图元的实例数，则：

$$
(f_G)_i=\#(g_i\subset G)
$$

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128212754710.png" style="zoom:33%;" />

- graphlet kernel可以定义为：

$$
K(G,G')=f_G^Tf_{G'}
$$

- 如果两张图的size不同会引起结果的失真，那么就可以进行正则化操作，令：

$$
h_G=\frac{f_g}{\sum f_G}\rightarrow K(G,G')=h_G^Th_{G'}
$$

- 计算图元的时间与图的大小呈现指数式关系。
- 这种kernel的问题在于计算graphlet的个数的时间复杂度是非常高的，并且是NP-hard的。

#### Weisfeiler-Lehman(威斯费勒-莱曼)核

Weisfeiler-Lehman图同构测试（颜色细化：Color Refinement）算法

给定：一个包含一组节点 $V$ 的图 $G$ 

- 为每个节点 $v$ 分配一个初始颜色$c^{(0)}(v)$

- 迭代地细化节点颜色：将邻近的颜色进行迭代聚合或散列来得到新的颜色。在这里，HASH将不同的输入映射为不同的颜色。

$$
c^{(k+1)}(v)=\operatorname{HASH}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\}\right)
$$

​		上式可以这样想，给定节点 $v$ 的新颜色将会是它自己在上一个步骤的颜色和节点 $v$ 的邻近节点 $u$ 的颜色串联的散列值。

- 经过 $K$ 步的色彩细化，$c^{(K)}(v)$ 总结出 K-hop neighborhood 的结构，即 K 步远的领域的图的结构。

**举例说明：给定两个图的颜色细化算法实例**

![](https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128231304824.png)

经过 k 步迭代的颜色细化后，WL内核对给定颜色的节点数量进行计数。因此，在这个例子中，我们进行了三次迭代，所以我们有13种的不同颜色。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128231719377.png" style="zoom:50%;" />

Weisfeiler-Lehman图核就是拿这两个特征描述向量的数量积作为两个图相似性的返回值。

<img src="https://gitee.com/shenhao-stu/picgo/raw/master/CS224W/image-20211128231955385.png" style="zoom:33%;" />

**优势：**

- 具有较高的计算效率
  - 颜色细化每个步骤的时间复杂度与图的大小**呈线性关系**。
  - 每个节点要做的全部，就只是收集邻近节点的颜色，然后用一个简单的散列函数来产生一个新的颜色。
- 当计算核值时，只有两个图中出现的颜色需要被追踪。因此颜色的数量最多是网络中节点的数量。
- 计算颜色同样也需要线性时间，因为这只是对节点进行扫视。
- 总而言之，计算一对图的Weisfeiler-Lehman图核的总时间复杂度只是与两个图的边的数量呈线性关系。

#### 小结

**Graphlet Kernel**

- 将图以图元袋 **Bag-of-graphlets** 表示
- 由于计算图元的时间与图的大小呈指数式关系，因此计算代价昂贵

**Weisfeiler-Lehman Kernel**

- 将图以颜色袋 **Bag-of-Colors** 表示

- 基于颜色细化算法

  - 在节点聚合邻近节点颜色后，丰富与创造新的节点颜色。

  - 随着这个颜色细化多次进行，节点收集颜色信息的部分，离图的中心就越远。

- 计算十分高效，其时间与图的大小呈线性关系。

同时，Weisfeiler-Lehman(威斯费勒-莱曼)核也和图神经网络有关，是一个度量图之间的相似度的很好的方式。



